{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cat Boost Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Einleitung\n",
    "\n",
    "- Yandex is Russian Google\n",
    "- Yandex uses \"gradient boosting\" a lot to power their services (music streaming, search, everything really)\n",
    "- Gradient boosting on decision trees is a form of machine learning that works by progressively training more complex models to maximize the accuracy of predictions. \n",
    "- It's particularly useful for predictive models that analyze ordered (continuous) data and categorical data. \n",
    "- It's  one of the most efficient ways to build ensemble models. The combination of gradient boosting with decision trees provides state-of-the-art results in many applications with structured data.\n",
    "- On the first iteration, the algorithm learns the first tree to reduce the training error, shown on left-hand image in figure 1. \n",
    "- This model usually has a significant error; it’s not a good idea to build very big trees in boosting since they overfit the data.\n",
    "- The right-hand image in figure 1 shows the second iteration, in which the algorithm learns one more tree to reduce the error made by the first tree. \n",
    "- The algorithm repeats this procedure until it builds a decent quality model\n",
    "\n",
    "![alt text](https://devblogs.nvidia.com/wp-content/uploads/2018/12/first-second-trees-625x357.png)\n",
    "\n",
    "\n",
    "- Gradient Boosting is a way to implement this idea for any continuous objective function. \n",
    "\n",
    "### Each step of Gradient Boosting combines two steps:\n",
    "\n",
    "- Step 1 - Computing gradients of the loss function we want to optimize for each input object\n",
    "- Step 2 - Learning the decision tree which predicts gradients of the loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-27T14:52:39.114440Z",
     "start_time": "2019-03-27T14:52:37.862697Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "#data visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import librosa\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from scipy.stats import kurtosis\n",
    "from scipy.stats import skew\n",
    "\n",
    "pd.options.display.precision = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Umgebungsvariablen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-27T14:52:42.241616Z",
     "start_time": "2019-03-27T14:52:42.237623Z"
    }
   },
   "outputs": [],
   "source": [
    "#feature generated 24.03.2019\n",
    "earthquake_daten = '''C:/studium/studium/CAS_PML/Projekt_Arbeit/earthquake/Daten/earthquake_data/'''\n",
    "feature_62900_94 = 'Features_62900-94.csv'\n",
    "feature_41934_94 = 'Features_41934_94.csv'\n",
    "Features_4194_94 = 'Features_4194_94.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature laden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-27T14:52:53.811258Z",
     "start_time": "2019-03-27T14:52:53.053249Z"
    }
   },
   "outputs": [],
   "source": [
    "train_data = pd.read_csv(earthquake_daten+feature_41934_94)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-27T14:52:55.025652Z",
     "start_time": "2019-03-27T14:52:55.005705Z"
    }
   },
   "outputs": [],
   "source": [
    "feature_data = train_data.iloc[:,1:95]\n",
    "time_to_failure = np.array(train_data.iloc[:,-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Filtern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-27T14:52:46.057876Z",
     "start_time": "2019-03-27T14:52:46.053887Z"
    }
   },
   "outputs": [],
   "source": [
    "items_to_filter = ['']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_feature_data = feature_data.filter(items=items_to_filter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_data = filtered_feature_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training / Testdaten vorbereiten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    feature_data,\n",
    "    time_to_failure,\n",
    "    random_state=0,\n",
    "    test_size=0.25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Workbench"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-27T14:53:00.794928Z",
     "start_time": "2019-03-27T14:52:59.509755Z"
    }
   },
   "outputs": [],
   "source": [
    "# Scaler\n",
    "\n",
    "\n",
    "# Model selection\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "# Modell\n",
    "from catboost import CatBoostRegressor, Pool"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quelle: https://www.analyticsvidhya.com/blog/2016/02/complete-guide-parameter-tuning-gradient-boosting-gbm-python/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-27T14:53:02.826139Z",
     "start_time": "2019-03-27T14:53:02.821121Z"
    }
   },
   "outputs": [],
   "source": [
    "def getMostImportendFeatuers(feat_imp, topAmount):\n",
    "    '''Gibt die wichtigsten Features zurück. Wird benötigt, um die Trainingsdaten und Testdaten \n",
    "    anhand Featuers zu filtern.'''\n",
    "    i = 0\n",
    "    featureFilter = list()\n",
    "    for feature in feat_imp:\n",
    "        featureFilter.append(feature)\n",
    "        i +=1\n",
    "        if i >= topAmount:\n",
    "            break\n",
    "    return featureFilter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-27T14:53:03.580297Z",
     "start_time": "2019-03-27T14:53:03.573775Z"
    }
   },
   "outputs": [],
   "source": [
    "def modelfit(alg, dtrain, time_to_failure, performCV=True, printFeatureImportance=True, cv_folds=5):\n",
    "    #Fit the algorithm on the data\n",
    "    alg.fit(dtrain, time_to_failure)\n",
    "      \n",
    "    #Predict training set:\n",
    "    dtrain_predictions = alg.predict(dtrain)\n",
    "\n",
    "    #Perform cross-validation:\n",
    "    if performCV:\n",
    "        cv_score = cross_val_score(alg, dtrain, time_to_failure, cv=cv_folds, scoring='neg_mean_absolute_error',n_jobs=-1)\n",
    "    \n",
    "    #Print model report:\n",
    "    print(\"\\nModel Report\")\n",
    "    print(\"MAE train data: %.4g\" % mean_absolute_error(time_to_failure, dtrain_predictions)) \n",
    "    \n",
    "    if performCV:\n",
    "        print(\"CV Score MAE: \\nMean %.7g \\nStd %.7g \\nMin %.7g \\nMax %.7g\" % (np.mean(cv_score),np.std(cv_score),np.min(cv_score),np.max(cv_score)))\n",
    "    \n",
    "    #Print Feature Importance:\n",
    "    if printFeatureImportance:\n",
    "        feat_imp = pd.Series(alg.feature_importances_, dtrain.columns).sort_values(ascending=False)\n",
    "        feat_imp.plot(kind='bar', title='Feature Importances',figsize=(15,10))\n",
    "        plt.ylabel('Feature Importance Score')\n",
    "        print('Top 30 Feature')\n",
    "        print(getMostImportendFeatuers(feat_imp.index,30))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-03-27T15:00:34.877Z"
    }
   },
   "outputs": [],
   "source": [
    "gbm0 = CatBoostRegressor(random_state=10, verbose = False,task_type = 'GPU')\n",
    "modelfit(gbm0, feature_data, time_to_failure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameter Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Tune tree based and boosting parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tree based parameter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fix learning rate and number of estimators for tuning tree-based parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Determine the optimum number of trees for fix learning rate 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_test1 = {'n_estimators':range(20,81,10)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fix learning rate and number of estimators for tuning tree-based parameters\n",
    "In order to decide on boosting parameters, we need to set some initial values of other parameters. Lets take the following values:\n",
    "\n",
    "min_samples_split = 500 : This should be ~0.5-1% of total values. Since this is imbalanced class problem, we’ll take a small value from the range.\n",
    "min_samples_leaf = 50 : Can be selected based on intuition. This is just used for preventing overfitting and again a small value because of imbalanced classes.\n",
    "max_depth = 8 : Should be chosen (5-8) based on the number of observations and predictors. This has 87K rows and 49 columns so lets take 8 here.\n",
    "max_features = ‘sqrt’ : Its a general thumb-rule to start with square root.\n",
    "subsample = 0.8 : This is a commonly used used start value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score='raise-deprecating',\n",
       "       estimator=GradientBoostingRegressor(alpha=0.9, criterion='friedman_mse', init=None,\n",
       "             learning_rate=0.1, loss='ls', max_depth=8,\n",
       "             max_features='sqrt', max_leaf_nodes=None,\n",
       "             min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "             min_samples_leaf=50, min_sa...       subsample=0.8, tol=0.0001, validation_fraction=0.1, verbose=0,\n",
       "             warm_start=False),\n",
       "       fit_params=None, iid=False, n_jobs=-1,\n",
       "       param_grid={'n_estimators': range(20, 81, 10)},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring='neg_mean_absolute_error', verbose=0)"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min_samples_split = int(filtered_feature_data.shape[0]*0.008)\n",
    "\n",
    "estimator = GradientBoostingRegressor(\n",
    "            learning_rate=0.1, \n",
    "            min_samples_split=500,\n",
    "            min_samples_leaf=50,\n",
    "            max_depth=8,\n",
    "            max_features='sqrt',\n",
    "            subsample=0.8,\n",
    "            random_state=10)\n",
    "\n",
    "gsearch1 = GridSearchCV(\n",
    "    estimator = estimator,\n",
    "    param_grid = param_test1, \n",
    "    scoring='neg_mean_absolute_error',\n",
    "    iid=False,\n",
    "    n_jobs=-1,\n",
    "    cv=5)\n",
    "                        \n",
    "gsearch1.fit(feature_data,time_to_failure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best params {'n_estimators': 30}\n",
      "Best MAE score: -2.1959848493751033\n"
     ]
    }
   ],
   "source": [
    "print('Best params {}'.format(gsearch1.best_params_))\n",
    "print('Best MAE score: {}'.format(gsearch1.best_score_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timestamp = time.time()\n",
    "#https://catboost.ai/docs/concepts/python-reference_catboost_save_model.html\n",
    "m.save_model(cat_boost_model+str(timestamp), \n",
    "           format=\"cbm\", \n",
    "           export_parameters=None,\n",
    "           pool=None)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "349.091px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
